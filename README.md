
## 개요 


![image](https://github.com/user-attachments/assets/286594d7-d6b4-4a3e-85ec-af8a87fcc203)
HRM모델을 고안한 Sapient Intelligence는 HRM모델이 ARC류 문제에 우수한 근거 중 하나로 다음과 같이 주장한다:  "H, L모듈간 학습 빈도로 인한 창발적인 역할 차이"

더 구체적으로는, 같은 구조 같은 파라미터의 두 모듈의 학습 빈도를 다르게 하는 방식으로 한쪽(H) 모듈은 뇌의 전두엽 피질처럼 고차원의 정보를 처리하도록 특화되었고, 나머지(L) 모듈은 뇌의 감각 피질처럼 저차원의 정보를 처리하도록 특화되었다고 주장한다. 

나는 "학습 방식만으로 같은 구조의 모듈에 서로 다른 문제 처리 영역을 분담할 수 있고 또 이런 분업화가 추론 성능 향상에 도움이 된다"라는 전제하에 새 프로젝트를 기획했다. 
분업화하는 모듈 수가 둘이 아니라 셋, 넷, 나아가 시스템이 직접 포화 상태 여부를 파악하고 새로운 모듈을 생성할 수 있다면, 또 MoE방식으로 이 새로운 모듈들을 문제 유형에 맞게 시스템이 선택할 수 있다면 Chollet 교수가 정의한 "완전히 새로운 문제에 명시적 설계 없이, 효율적으로 새로운 기술을 얻고 독창적인 해법을 찾는 능력"을 갖춘 AI 시스템이 될 수 있을까 하는 호기심에 이 프로젝트를 도전했다. 

## 아키텍처

![image](https://github.com/user-attachments/assets/4d6e9225-b0e6-4a4e-87d8-f4ace0523e05)
아키텍처는 위와 같다. 기존 HRM에서의 H모듈을 Frontal Module이라고 재정의하고, L모듈을 Reason Module이라고 재정의한다. 

순전파 과정에 이 두 모듈이 상호작용을 하는 방식은 HRM과 같다. 

$$z_{r'}^{(t+1)}=f_{r'}(z_{r'}^{(t)}+z_f^{(t)}+input\_embedding)\ \ (r'는\ 활성화된\ Reason\ module)$$

$$z_f^{(t+1)}=f_f(z_{r'}^{(t)}+z_f^{(t)})$$

마찬가지로, HRM과 같은 input, output 모델 구조를 공유하고 HRM과 같은 H-L Cycle 메커니즘과 1-step BPTT 방식을 공유한다. 

![image](https://github.com/user-attachments/assets/653cdfd1-11d2-42e4-8e10-c39da7ea4b39)

HRM과 ERM의 아키텍쳐에서 차이점은 단 두 가지다. 첫째로, L-level 모듈(reason 모듈)이 하나가 아니라 최대 8개까지 존재할 수 있다는 점이다. 이때, 동시에 여러 reason 모듈이 활성화되진 않고 단 하나의 모듈만 활성화되어 기존 HRM의 H-L모듈 상호작용 과정을 해치지 않고, 각 reason모듈이 할당된 문제 상황을 전담할 수 있게 한다.

둘째로, 기존 ACT(강화학습 방식으로 생각의 길이를 결정하는 HRM의 메커니즘)모듈에 어떤 Reason 모듈을 선택할지 결정하는 라우팅 기능이 추가되었다는 점이다. (thalamus module이라고 명명) 라우팅 모듈을 구현하는 방식으로는 좀 더 학습이 쉬운 각 모듈의 정답률을 예측하는 지도 학습 신경망 방식과 이산적인 모듈 선택을 정책 학습으로 미분 가능케 해주는 REINFORCE 방식 두 가지 옵션이 있다.

## 학습전략

안정적인 역할 분리를 위해서 ERM 모델의 학습 전략은 다음 세 단계를 따른다. 

1. Stablilization Phase (안정 학습 시기)
2. Adaptive Phase (적응 학습 시기)
3. Convergence Check(수렴 확인 시기)


처음에 모델은 하나의 reason 모듈만 가진 체로, HRM과 완전히 같은 구조로 학습을 시작한다. 맨 처음엔 하이퍼 파라미터로 정해진 스텝 수만큼 첫 reason 모듈이 문제를 할당받아 첫 reason 모듈을 어느 정도 수렴시킨다. 

첫 모듈이 어느 정도 수렴된 후 두 번째 모듈이 생성되며 비로소 게이팅 모듈이 제대로 학습되기 시작하며 안정 학습 시기에 돌입한다. 

안정 학습 시기에는 하이퍼 파라미터로 미리 정한 비율에 해당하는 만큼의 미니 배치 내 문제 비율이 강제로 새로 생성된 모듈에 할당된다. 이때, 새 모듈이 할당할 문제는 랜덤하게 선택되는 것이 아니라 기존 잘 학습된 게이팅 모듈이 추측한 각 문제당 기존 모듈들의 정답률을 활용해 선택한다. 각 문제의 기존 모듈들의 정답률 최댓값을 해당 문제의 난이도로 정의하고, 미니 배치 내 문제들을 난이도별로 정렬한 후 난이도 상위 n%의 문제들이 새로 생성된 모듈에 할당되어 기존 시스템이 어려워하는 문제 유형을 새 모듈이 담당할 수 있도록 한다.
이 단계에선 분산이 매우 큰 분산의 그래디언트가 흘러 들어오므로 기존에 시스템이 학습한 내용이 붕괴할 수 있어 기존 모듈들(thalamus, frontal 모듈)은 학습되지 않거나, 아주 낮은 학습률로 학습된다. 

안정 학습 후 어느 정도 새로운 모듈이 문제에 특화되었다면, 그다음은 게이팅 모듈이 새로운 모듈에 적절히 문제를 할당시키도록 학습하기 위한 적응 학습 시기로 넘어간다. 새 모듈에 문제를 강제 할당하진 않지만, 로짓에 보너스를 더해주거나 정책의 선택 확률에 보너스를 주거나 하는 방식으로 새 모듈에 문제를 할당하는 것이 어느 정도 유리하다고 게이팅 모듈이 판단하도록 학습을 진행한다. 이 시기에는 기존 모듈들도 함께 학습된다.

적응학습이 끝나 게이팅 모듈과 새로운 reason모듈이 모두 시스템에 적응했다면 다음은 수렴 확인 시기로 넘어간다. 수렴 확인 시기에는 게이팅 모듈에 흘러 들어오는 역전파 그래디언트의 분산을 이동평균으로 추적해 시스템의 안정화 정도를 추적한다. 이 외에 모든 모듈은 어떠한 인위적 조정 없이 정상적으로 학습되며, 역전파 그래디언트의 분산이 미리 하이퍼파라미터로 정한 임계 아래로 내려가면, 더 이상 기존 모듈들로는 새로운 문제에 적응하기 어렵고, 시스템이 포화했다고 판단해 새로운 reason 모듈을 생성한 후 안정 학습부터 반복한다. 안정 학습과 적응 학습 기간은 하이퍼파라미터로 미리 정한다. 

## 실험 결과(실패 분석)

하이퍼파라미터들을 이래저래 조정하고 게이팅 라우터 옵션도 바꿔가며 실험을 해봤는데 다음과 같은 일관적인 결과를 얻을 수 있었다.

![image](https://github.com/user-attachments/assets/4d270211-f6a0-4c4d-95d4-4f7e823ef205)
![image](https://github.com/user-attachments/assets/17b71272-afa4-4b29-95b9-9b7ec58ae89a)
![image](https://github.com/user-attachments/assets/6e9db79a-905b-4d7c-9360-55fca255d777)
1. 불안정한 성능.

직접 HRM을 학습해 본 결과 HRM의 학습 그래프는 다음과 같이 유사도와 정확도가 함께 안정적으로 상승하며 학습되는 양상을 보인다

![image](https://github.com/user-attachments/assets/bf8c12c3-29b6-49a9-a566-db23a77b1da0)

반면, 내가 고안한 아키텍처는 어떤 옵션에도 저렇게 안정적인 분산으로 정확도와 유사도가 함께 오르지 않았다. 불안정한 구조 탓인지 전반적인 그래디언트의 분산도 훨씬 컸고 사실상 학습에 실패했다. 

2. reason 모듈 고착화

하이퍼파라미터를 잘 조정했을 때는 3 phase의 학습 단계가 잘 동작해 모듈이 잘 생성되는 것을 볼 수 있었지만, 안정 학습과 적응 학습이 끝나고 나면 어김없이 맨 첫 모듈만 활성화되는 방향으로 시스템이 고착화되었다. 각 하위 모듈이 서로 다른 문제를 할당하게끔 하자는 게 의도였는데, 이런 현상은 시스템이 의도대로 동작하지 않는다는 것을 의미한다. 

그렇다면 왜 이런 실패를 겪었을까.

1. 복잡한 모델 구조

![image](https://github.com/user-attachments/assets/b695cefa-284b-4267-b972-15b7bcd509ee)

이런저런 기능을 추가하려고 한 탓에 하이퍼파라미터가 지나치게 많아졌다.
실험 하나하나가 무거운데 이런 넓은 탐색공간에서 휴리스틱한 방식으로 최적 조합을 찾는 것은 사실상 불가능에 가깝다. 

2. ARC문제의 복잡성

ARC문제는 구조적으로 아주 분산이 큰 데이터셋에 속한다. 단순한 통계적인 방식이나 단순한 머신러닝으로는 문제에 내재한 본질적 논리구조를 포착할 수 없는데, 이 아키텍처의 게이팅 모듈은 이 복잡한 ARC문제를 분류해야 하는 임무를 맡는다. 이런 상황에서 시스템이 올바르게 동작하길 기대하는 건 그다지 현명하지 못한 것 같다.

3. 실험 환경 부족

내가 가진 실험 자원이라고 해봤자 코랩프로+에서 제공하는 1코어 H100정도가 한계다. 바닐라 HRM도 통째로 올리지 못해서 파라미터 반의반 토막으로 줄이고 데이터도 줄여서 겨우 학습했다. 
동적으로 파라미터가 증가하는 이 시스템을 학습하기 위해서 바닐라 HRM을 경량화한 것보다 더 많이 경량화해서 구현했는데, 지나치게 모델이 작아지면서 유의미한 학습에 실패한 것일 수 있다. 
나름 개선해 보자고 A100도 하나 빌려봤는데 결과가 크게 달라지지 않았다. 

4. 코딩 실력 부족

그냥 내 코딩 실력이 부족해서 그런 것일 수도 있다. 내가 기존에 다뤄본 프로젝트라고 해봤자 단일 코드 내에서 모델 구현하는 정도에 그쳤는데 이런 모듈화된 프로젝트를 내 목적에 리팩토링하는 작업이 한 번에 성공할 가능성이 그리 높지 않다. (심지어 HRM코드는 그다지 잘 모듈화된 편도 아니다. 데이터 처리 파이프라인이나 학습 파이프라인 관련 코드들이 여기저기 흩어져있다...) 그래서 나름 코드 스터디를 하긴 했지만, 대부분의 코딩을 바이브 코딩의 도움받아 해서 만든 이 코드가 온전히 내 의도에 맞게 동작할 만큼 완성도 높은 코드가 아니었을 수 있다.

이런 결과를 얻고 회의감 아닌 회의감을 갖고 있는 와중 다음 포스팅들을 보았다. 

![image](https://github.com/user-attachments/assets/07a6ec0e-2b72-4c48-a382-fff9b2fbf340)

첫째로 ARC 저자 측이 재현 실험과 비교 실험을 하며 얻은 결과를 포스팅한 블로그다. 대충 요약하자면, HRM이 우수한 이유는 sapientinc측이 주장한 모듈들의 역할 분리가 아닌 한 문제에 관한 결과를 내기까지 수백 번에 달하는 순전파를 통해 정답을 탐색하는 딥 슈퍼비전에 있다는 것이다. 

![image](https://github.com/user-attachments/assets/3033636d-3bab-4d24-996c-b309c2877aee)

두 번째론 이런 ARC측의 실험 결과를 입증이라도 하듯 나온 논문 한 편이다. 삼성에서 만든 TRM이라는 논문인데, 내부 모듈을 하나로 줄여 경량화한 대신 HRM의 깊은 내부 추론 루프인 딥 슈퍼비전에 주목해 HRM보다 더 우수한 성능을 얻은 TRM이라는 프로젝트 관련 논문이다. 

![image](https://github.com/user-attachments/assets/6525d059-4c0a-4f18-9b74-eed2d17f217f))

심지어 이 논문의 맨 뒷단에 나오는 실패한 아이디어 목록 첫 줄에 MoE방식이 실패했다고 나왔다! 

아무튼 이런 내용들로 내 프로젝트의 전제였던 "학습 방식만으로 같은 구조의 모듈에 서로 다른 문제 처리 영역을 분담할 수 있고 또 이런 분업화가 추론 성능 향상에 도움이 된다"가 틀렸다고 판단해서 프로젝트를 마쳤다. 

씁쓸한 실패지만 그래도 재밌었고 좋은 경험이 되었다. 
